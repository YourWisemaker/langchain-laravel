# LangChain Laravel Package Environment Configuration

# =============================================================================
# LANGCHAIN LARAVEL PACKAGE CONFIGURATION
# =============================================================================

# -----------------------------------------------------------------------------
# Default AI Provider Configuration
# -----------------------------------------------------------------------------
# Choose your default AI provider: openai, claude, llama
LANGCHAIN_DEFAULT_PROVIDER=openai

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------
# Your OpenAI API key (required for OpenAI)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: OpenAI Organization ID
# OPENAI_ORGANIZATION=your_organization_id

# Optional: Custom OpenAI base URL
# OPENAI_BASE_URL=https://api.openai.com/v1

# Default OpenAI model to use
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo

# Default maximum tokens for OpenAI requests
OPENAI_DEFAULT_MAX_TOKENS=1000

# Default temperature for OpenAI requests
OPENAI_DEFAULT_TEMPERATURE=0.7

# -----------------------------------------------------------------------------
# Claude (Anthropic) Configuration
# -----------------------------------------------------------------------------
# Your Claude API key (required for Claude)
CLAUDE_API_KEY=your_claude_api_key_here

# Claude API base URL
CLAUDE_BASE_URL=https://api.anthropic.com

# Claude API version
CLAUDE_API_VERSION=2023-06-01

# Default Claude model to use
CLAUDE_DEFAULT_MODEL=claude-3-sonnet-20240229

# Default maximum tokens for Claude requests
CLAUDE_DEFAULT_MAX_TOKENS=1000

# Default temperature for Claude requests
CLAUDE_DEFAULT_TEMPERATURE=0.7

# -----------------------------------------------------------------------------
# Llama Configuration
# -----------------------------------------------------------------------------
# Your Llama API key (required for Llama - e.g., Together AI)
LLAMA_API_KEY=your-llama-api-key-here

# Llama API base URL (e.g., Together AI, Replicate, etc.)
LLAMA_BASE_URL=http://localhost:11434

# Default Llama model to use
LLAMA_DEFAULT_MODEL=llama2

# Default maximum tokens for Llama requests
LLAMA_DEFAULT_MAX_TOKENS=1000

# Default temperature for Llama requests
LLAMA_DEFAULT_TEMPERATURE=0.7

# -----------------------------------------------------------------------------
# DeepSeek Configuration
# -----------------------------------------------------------------------------
# Your DeepSeek API key (required for DeepSeek)
DEEPSEEK_API_KEY=your-deepseek-api-key-here

# DeepSeek API base URL
DEEPSEEK_BASE_URL=https://api.deepseek.com

# Default DeepSeek model to use
DEEPSEEK_DEFAULT_MODEL=deepseek-chat

# Default maximum tokens for DeepSeek requests
DEEPSEEK_DEFAULT_MAX_TOKENS=1000

# Default temperature for DeepSeek requests
DEEPSEEK_DEFAULT_TEMPERATURE=0.7

# =============================================================================
# LangChain Cache Configuration
# =============================================================================

# Enable/disable caching for API responses
LANGCHAIN_CACHE_ENABLED=true

# Cache TTL (Time To Live) in seconds
# 3600 = 1 hour, 86400 = 1 day
LANGCHAIN_CACHE_TTL=3600

# Cache store to use (default uses Laravel's default cache store)
# Options: file, redis, memcached, database, etc.
LANGCHAIN_CACHE_STORE=file

# Cache key prefix
LANGCHAIN_CACHE_PREFIX=langchain

# =============================================================================
# Laravel Application Configuration
# =============================================================================

# Application environment
APP_ENV=local

# Application debug mode
APP_DEBUG=true

# Application URL
APP_URL=http://localhost

# =============================================================================
# Database Configuration (for testing)
# =============================================================================

# Database connection
DB_CONNECTION=sqlite
DB_DATABASE=:memory:

# =============================================================================
# Cache Configuration
# =============================================================================

# Cache driver
CACHE_DRIVER=file

# =============================================================================
# Session Configuration
# =============================================================================

# Session driver
SESSION_DRIVER=file

# =============================================================================
# Queue Configuration
# =============================================================================

# Queue connection
QUEUE_CONNECTION=sync

# =============================================================================
# Mail Configuration (for notifications)
# =============================================================================

# Mail driver
MAIL_MAILER=log

# =============================================================================
# Logging Configuration
# =============================================================================

# Log channel
LOG_CHANNEL=stack

# Log level
LOG_LEVEL=debug

# =============================================================================
# Testing Configuration
# =============================================================================

# Enable/disable integration tests with real API calls
# Set to false to skip tests that require actual OpenAI API calls
RUN_INTEGRATION_TESTS=false

# Test API key (can be different from production)
TEST_OPENAI_API_KEY=your_test_api_key_here

# =============================================================================
# Performance Configuration
# =============================================================================

# Enable/disable performance monitoring
LANGCHAIN_PERFORMANCE_MONITORING=true

# Maximum concurrent requests
LANGCHAIN_MAX_CONCURRENT_REQUESTS=10

# Request timeout in seconds
LANGCHAIN_REQUEST_TIMEOUT=30

# =============================================================================
# Security Configuration
# =============================================================================

# Enable/disable request logging (be careful with sensitive data)
LANGCHAIN_LOG_REQUESTS=false

# Enable/disable response logging
LANGCHAIN_LOG_RESPONSES=false

# Maximum prompt length (characters)
LANGCHAIN_MAX_PROMPT_LENGTH=10000

# =============================================================================
# Rate Limiting Configuration
# =============================================================================

# Enable/disable rate limiting
LANGCHAIN_RATE_LIMITING_ENABLED=true

# Requests per minute limit
LANGCHAIN_RATE_LIMIT_PER_MINUTE=60

# Rate limit cache store
LANGCHAIN_RATE_LIMIT_STORE=cache

# =============================================================================
# Content Moderation Configuration
# =============================================================================

# Enable/disable content moderation
LANGCHAIN_CONTENT_MODERATION_ENABLED=false

# Content moderation service
# Options: openai, custom
LANGCHAIN_CONTENT_MODERATION_SERVICE=openai

# =============================================================================
# Batch Processing Configuration
# =============================================================================

# Maximum batch size for bulk operations
LANGCHAIN_MAX_BATCH_SIZE=50

# Batch processing timeout in seconds
LANGCHAIN_BATCH_TIMEOUT=300

# =============================================================================
# Development Configuration
# =============================================================================

# Enable/disable debug mode for LangChain
LANGCHAIN_DEBUG=false

# Enable/disable verbose logging
LANGCHAIN_VERBOSE_LOGGING=false

# Mock API responses for development
LANGCHAIN_MOCK_RESPONSES=false

# =============================================================================
# Webhook Configuration (if using webhooks)
# =============================================================================

# Webhook URL for notifications
LANGCHAIN_WEBHOOK_URL=

# Webhook secret for verification
LANGCHAIN_WEBHOOK_SECRET=

# =============================================================================
# Analytics Configuration
# =============================================================================

# Enable/disable usage analytics
LANGCHAIN_ANALYTICS_ENABLED=true

# Analytics provider
# Options: internal, external
LANGCHAIN_ANALYTICS_PROVIDER=internal

# =============================================================================
# Backup Configuration
# =============================================================================

# Enable/disable automatic backups of important data
LANGCHAIN_BACKUP_ENABLED=false

# Backup storage disk
LANGCHAIN_BACKUP_DISK=local

# =============================================================================
# Multi-tenant Configuration (if applicable)
# =============================================================================

# Enable/disable multi-tenant support
LANGCHAIN_MULTI_TENANT=false

# Tenant identification method
# Options: subdomain, header, parameter
LANGCHAIN_TENANT_METHOD=subdomain

# =============================================================================
# Custom Configuration
# =============================================================================

# Add your custom configuration variables here
# CUSTOM_VARIABLE=value

# =============================================================================
# Notes
# =============================================================================

# 1. Never commit your actual API keys to version control
# 2. Use different API keys for development, testing, and production
# 3. Consider using Laravel's encryption for sensitive configuration
# 4. Review and adjust cache settings based on your usage patterns
# 5. Monitor your OpenAI usage and costs regularly
# 6. Test your configuration in a development environment first
# 7. Keep your OpenAI PHP client library updated
# 8. Consider implementing proper error handling and logging
# 9. Use environment-specific configuration files when needed
# 10. Document any custom configuration for your team